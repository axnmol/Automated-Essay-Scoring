{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data\n",
    "Performing attribute reduction by removing empty columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------Essay Set Descriptions----------\n",
      "\n",
      "   essay_set                         type_of_essay  training_set_size\n",
      "0          1  persuasive / narrative  / expository               1783\n",
      "1          2  persuasive / narrative  / expository               1800\n",
      "2          3            source dependent responses               1726\n",
      "3          4            source dependent responses               1772\n",
      "4          5            source dependent responses               1805\n",
      "5          6            source dependent responses               1800\n",
      "6          7  persuasive / narrative  / expository               1569\n",
      "7          8  persuasive / narrative  / expository                723\n",
      "\n",
      "Minimum scores for each essay set:\n",
      "[2, 1, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Maximum scores for each essay set:\n",
      "[12, 6, 3, 3, 4, 4, 30, 60]\n"
     ]
    }
   ],
   "source": [
    "DATASET_DIR = './data/' # Datasets to be places here\n",
    "# pip install pandas\n",
    "# pip install xlrd\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# removing empty columns and finding minimum and maximum scores given to each of the 8 sets\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "# reading from tsv file (tab - separated) with Latin alphabet encoding including special symbols\n",
    "Y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])\n",
    "Z = pd.read_excel(r'./data/essay_set_descriptions.xlsx')\n",
    "\n",
    "# minimum and maximum scores for each dataset\n",
    "minimum_scores = Z['min_domain1_score'].to_list()\n",
    "minimum_scores.insert(0,-1)\n",
    "maximum_scores = Z['max_domain1_score'].to_list()\n",
    "maximum_scores.insert(0,-1)\n",
    "print('\\n----------Essay Set Descriptions----------\\n')\n",
    "print(Z[['essay_set','type_of_essay','training_set_size']])\n",
    "\n",
    "print('\\nMinimum scores for each essay set:')\n",
    "print(minimum_scores[1:9])\n",
    "print('\\nMaximum scores for each essay set:')\n",
    "print(maximum_scores[1:9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Dataframe\n",
    "Our main dataframe consists of 12975 sample essays which goes upto essay_id 21633 and are divided into 8 sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12971</th>\n",
       "      <td>21626</td>\n",
       "      <td>8</td>\n",
       "      <td>In most stories mothers and daughters are eit...</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12972</th>\n",
       "      <td>21628</td>\n",
       "      <td>8</td>\n",
       "      <td>I never understood the meaning laughter is th...</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12973</th>\n",
       "      <td>21629</td>\n",
       "      <td>8</td>\n",
       "      <td>When you laugh, is @CAPS5 out of habit, or is ...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12974</th>\n",
       "      <td>21630</td>\n",
       "      <td>8</td>\n",
       "      <td>Trippin' on fen...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12975</th>\n",
       "      <td>21633</td>\n",
       "      <td>8</td>\n",
       "      <td>Many people believe that laughter can improve...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12976 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       essay_id  essay_set                                              essay  \\\n",
       "0             1          1  Dear local newspaper, I think effects computer...   \n",
       "1             2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2             3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3             4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4             5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "...         ...        ...                                                ...   \n",
       "12971     21626          8   In most stories mothers and daughters are eit...   \n",
       "12972     21628          8   I never understood the meaning laughter is th...   \n",
       "12973     21629          8  When you laugh, is @CAPS5 out of habit, or is ...   \n",
       "12974     21630          8                                 Trippin' on fen...   \n",
       "12975     21633          8   Many people believe that laughter can improve...   \n",
       "\n",
       "       domain1_score  \n",
       "0                  8  \n",
       "1                  9  \n",
       "2                  7  \n",
       "3                 10  \n",
       "4                  8  \n",
       "...              ...  \n",
       "12971             35  \n",
       "12972             32  \n",
       "12973             40  \n",
       "12974             40  \n",
       "12975             40  \n",
       "\n",
       "[12976 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of the Data\n",
    "\n",
    "These are all helper functions used to clean and tokenize the essays into sentences and wordlists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install gensim for Word2vec and Fasttext Model\n",
    "# !pip install nltk for natural Language Processing \n",
    "\n",
    "import nltk\n",
    "# first time run download these packages\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('popular')\n",
    "\n",
    "import re # for regular expressions operations \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    # remove the tagged labels and word tokenize the sentence\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v) # removing anything that is not alphabetic\n",
    "    words = essay_v.lower().split() # turn sentence into lowercase and split it into words \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\")) # english stopwords library \n",
    "        words = [w for w in words if not w in stops] \n",
    "        # words present in the sentence and not present in stopwords\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    # sentence tokenize the essay and call essay_to_wordlist() for word tokenization\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') # nltk library\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip()) # call tokenizer on essay striped of spaces \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "            # remove_stopwords carried forward to essay_to_wordlist it is a bool variable\n",
    "    return sentences # will be returning list of tokenized sentences without stopwords\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # make Feature Vector from the words list of an Essay\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #  make null vectors of length = num_features for every word index\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word) # vocabulary set of model\n",
    "    for word in words:\n",
    "        if word in index2word_set: # for word in a sentence being in index2word_set\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model.wv[word])\n",
    "    if num_words:\n",
    "        featureVec = np.divide(featureVec,num_words) \n",
    "        # divides every element of featureVec by num_words (to compute average) \n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    # main function to generate the word vectors for word2vec model\n",
    "    # it then calls makeFeatureVec and for every essay sentance\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    # make null vectors of length = length of essay having null vector of num_features at every index\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs\n",
    "    # then returns the trained model feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embeddings\n",
    "We will preprocess all essays and convert them to feature vectors using Word2vec and Fasttext model. We would also perform transfer learning by using pre-trained Word2vec, GloVe and Fasttext models.\n",
    "\n",
    "All the word embeddings used are 300 dimentional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing variables for word2vec and fasttext model.\n",
    "num_features = 300 # vector length\n",
    "min_word_count = 40 # to be considered for vectorisation\n",
    "num_workers = 8 # working cores\n",
    "context = 10\n",
    "downsampling = 1e-3 # compressing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Model\n",
    "Using word2vec model to make embeddings for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.093336</td>\n",
       "      <td>-0.045639</td>\n",
       "      <td>0.003970</td>\n",
       "      <td>0.031609</td>\n",
       "      <td>0.012581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053283</td>\n",
       "      <td>-0.043948</td>\n",
       "      <td>0.028168</td>\n",
       "      <td>-0.124830</td>\n",
       "      <td>-0.030559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.062648</td>\n",
       "      <td>-0.030446</td>\n",
       "      <td>-0.005180</td>\n",
       "      <td>-0.004587</td>\n",
       "      <td>-0.008295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>-0.063334</td>\n",
       "      <td>-0.018510</td>\n",
       "      <td>-0.062218</td>\n",
       "      <td>-0.012708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.059883</td>\n",
       "      <td>-0.009011</td>\n",
       "      <td>0.013409</td>\n",
       "      <td>0.068408</td>\n",
       "      <td>-0.005265</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081692</td>\n",
       "      <td>-0.041097</td>\n",
       "      <td>0.002538</td>\n",
       "      <td>-0.120984</td>\n",
       "      <td>-0.020349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.019802</td>\n",
       "      <td>-0.023876</td>\n",
       "      <td>-0.072023</td>\n",
       "      <td>0.028865</td>\n",
       "      <td>-0.067879</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039249</td>\n",
       "      <td>-0.065322</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>-0.004138</td>\n",
       "      <td>-0.027871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.134069</td>\n",
       "      <td>-0.119417</td>\n",
       "      <td>0.063802</td>\n",
       "      <td>0.087960</td>\n",
       "      <td>-0.047054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010881</td>\n",
       "      <td>-0.015373</td>\n",
       "      <td>-0.119925</td>\n",
       "      <td>-0.051917</td>\n",
       "      <td>0.003321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2897</th>\n",
       "      <td>-0.029500</td>\n",
       "      <td>-0.041040</td>\n",
       "      <td>0.038128</td>\n",
       "      <td>-0.017910</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.006357</td>\n",
       "      <td>0.005293</td>\n",
       "      <td>-0.070519</td>\n",
       "      <td>0.044967</td>\n",
       "      <td>0.096494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>-0.082162</td>\n",
       "      <td>0.037860</td>\n",
       "      <td>-0.034531</td>\n",
       "      <td>0.010890</td>\n",
       "      <td>0.050583</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045344</td>\n",
       "      <td>-0.074076</td>\n",
       "      <td>0.012369</td>\n",
       "      <td>-0.017194</td>\n",
       "      <td>0.063105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2899</th>\n",
       "      <td>-0.014986</td>\n",
       "      <td>-0.003020</td>\n",
       "      <td>-0.075203</td>\n",
       "      <td>0.040039</td>\n",
       "      <td>0.057193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>-0.038743</td>\n",
       "      <td>0.081464</td>\n",
       "      <td>-0.060517</td>\n",
       "      <td>0.013883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2900</th>\n",
       "      <td>0.023518</td>\n",
       "      <td>-0.009254</td>\n",
       "      <td>-0.087496</td>\n",
       "      <td>0.071146</td>\n",
       "      <td>0.017753</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010846</td>\n",
       "      <td>-0.004426</td>\n",
       "      <td>0.036070</td>\n",
       "      <td>0.033182</td>\n",
       "      <td>-0.021839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2901</th>\n",
       "      <td>0.050168</td>\n",
       "      <td>-0.020081</td>\n",
       "      <td>-0.023354</td>\n",
       "      <td>0.031540</td>\n",
       "      <td>-0.038394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024716</td>\n",
       "      <td>-0.005049</td>\n",
       "      <td>-0.018543</td>\n",
       "      <td>-0.004885</td>\n",
       "      <td>0.060112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2902 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4    ...       295  \\\n",
       "0    -0.093336 -0.045639  0.003970  0.031609  0.012581  ...  0.053283   \n",
       "1    -0.062648 -0.030446 -0.005180 -0.004587 -0.008295  ...  0.043100   \n",
       "2    -0.059883 -0.009011  0.013409  0.068408 -0.005265  ...  0.081692   \n",
       "3    -0.019802 -0.023876 -0.072023  0.028865 -0.067879  ...  0.039249   \n",
       "4    -0.134069 -0.119417  0.063802  0.087960 -0.047054  ...  0.010881   \n",
       "...        ...       ...       ...       ...       ...  ...       ...   \n",
       "2897 -0.029500 -0.041040  0.038128 -0.017910  0.002117  ... -0.006357   \n",
       "2898 -0.082162  0.037860 -0.034531  0.010890  0.050583  ... -0.045344   \n",
       "2899 -0.014986 -0.003020 -0.075203  0.040039  0.057193  ...  0.010275   \n",
       "2900  0.023518 -0.009254 -0.087496  0.071146  0.017753  ... -0.010846   \n",
       "2901  0.050168 -0.020081 -0.023354  0.031540 -0.038394  ...  0.024716   \n",
       "\n",
       "           296       297       298       299  \n",
       "0    -0.043948  0.028168 -0.124830 -0.030559  \n",
       "1    -0.063334 -0.018510 -0.062218 -0.012708  \n",
       "2    -0.041097  0.002538 -0.120984 -0.020349  \n",
       "3    -0.065322  0.004745 -0.004138 -0.027871  \n",
       "4    -0.015373 -0.119925 -0.051917  0.003321  \n",
       "...        ...       ...       ...       ...  \n",
       "2897  0.005293 -0.070519  0.044967  0.096494  \n",
       "2898 -0.074076  0.012369 -0.017194  0.063105  \n",
       "2899 -0.038743  0.081464 -0.060517  0.013883  \n",
       "2900 -0.004426  0.036070  0.033182 -0.021839  \n",
       "2901 -0.005049 -0.018543 -0.004885  0.060112  \n",
       "\n",
       "[2902 rows x 300 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec # for word2vec\n",
    "from gensim.test.utils import get_tmpfile # for saving model\n",
    "\n",
    "allsentences = [] # list of all sentences\n",
    "\n",
    "for essay in X['essay']:\n",
    "# obtaining all sentences from the essays.\n",
    "    allsentences += essay_to_sentences(essay, remove_stopwords=True)\n",
    "\n",
    "# for visualization of vectors\n",
    "visualmodel = Word2Vec(\n",
    "    allsentences, \n",
    "    workers=num_workers, \n",
    "    size=num_features, \n",
    "    min_count=min_word_count, \n",
    "    window=context, \n",
    "    sample=downsampling\n",
    ")\n",
    "\n",
    "path = get_tmpfile(\"./word2vecvis/word2vecvisual.model\") # path for the model \n",
    "\n",
    "visualmodel.init_sims(replace=True) # normalized model\n",
    "# different ways to save model\n",
    "visualmodel.wv.save_word2vec_format('./word2vecvis/word2vecvisual.txt', binary=False)\n",
    "visualmodel.wv.save_word2vec_format('./word2vecvis/word2vecvisual.bin', binary=True)\n",
    "visualmodel.save(\"./word2vecvis/word2vecvisual.model\")\n",
    "\n",
    "# embedding dataframe\n",
    "M=visualmodel.wv[visualmodel.wv.vocab]\n",
    "df=pd.DataFrame(M)\n",
    "pd.options.display.max_columns=10\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Word2Vec Model\n",
    "It is trained on the Google News dataset (about 100 billion words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "modelbasic = Word2Vec.load('./word2vecvis/word2vecvisual.model')\n",
    "# getting embeddings from pretrained word2vec model \n",
    "modelbasic.intersect_word2vec_format('./word2vec/word2vec.bin', binary=True, lockf=1.0)\n",
    "\n",
    "# will be using on training data\n",
    "# modelbasic.train(allsentences,total_examples=len(allsentences), epochs=modelbasic.iter)\n",
    "# modelbasic.init_sims(replace=True) # for normalizing\n",
    "# modelbasic.save(\"./word2vec/word2vecPre.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Glove Model\n",
    "It is trained on Wikipedia data and contains about 6 billion words in its vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "modelglove = Word2Vec.load('./word2vecvis/word2vecvisual.model')\n",
    "# Converting Glove to Word2Vec\n",
    "_ = glove2word2vec('./glove/glove.6B.300d.txt', \"./glove/gloveW2V.txt\")\n",
    "# Getting embeddings from pretrained Glove model\n",
    "modelglove.intersect_word2vec_format('./glove/gloveW2V.txt', binary=False, lockf=1.0)\n",
    "\n",
    "# will be using on training data\n",
    "# modelglove.train(allsentences,total_examples=len(allsentences), epochs=modelbasic.iter)\n",
    "# modelglove.init_sims(replace=True) for normalizing\n",
    "# modelglove.save(\"./glove/gloveW2V.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fasttext\n",
    "It is similar to word2vec model but also contains embeddings for n-grams which hep in data sets with out of vocabulary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "ftmodel = FastText(\n",
    "    allsentences, \n",
    "    workers=num_workers, \n",
    "    size=num_features, \n",
    "    min_count = min_word_count, \n",
    "    window = context, \n",
    "    sample = downsampling\n",
    ")\n",
    "# ftmodel.save(\"./fasttext/fasttext.model\") for saving with ngrams\n",
    "\n",
    "# embedding dataframe\n",
    "F=ftmodel.wv[ftmodel.wv.vocab]\n",
    "ft=pd.DataFrame(F)\n",
    "pd.options.display.max_columns=10\n",
    "ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Trained Fasttext Model\n",
    "It is the fasttext model pre-trained on Wiki-news also containing sub-words data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftmodelPre = Word2Vec.load('./word2vecvis/word2vecvisual.model')\n",
    "total_examples = ftmodelPre.corpus_count\n",
    "# Getting embeddings from pretrained Fasttext model \n",
    "ftmodelPre.intersect_word2vec_format('./fasttext/wiki-news-300d-1M-subword.vec')\n",
    "\n",
    "# will be using on training data\n",
    "# ftmodelPre.train(allsentences,total_examples=len(allsentences), epochs=ftmodel.iter)\n",
    "# ftmodelPre.save(\"./fasttext/fasttextPre.model\") for saving with ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "Files needed for visualization on Embedding Projector, Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Storing Visual model tsv files\n",
    "import io # for input output\n",
    "\n",
    "# files needed for tensorboard\n",
    "out_v = io.open('./word2vecvis/vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('./word2vecvis/meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "# write meta file and vector file\n",
    "for index in range(len(visualmodel.wv.index2word)): # for every word in vocab\n",
    "    word = visualmodel.wv.index2word[index]\n",
    "    vec = visualmodel.wv.vectors[index]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "# Open http://projector.tensorflow.org/\n",
    "# Click “Load Data” button from the left menu.\n",
    "# Select “Choose file” in “Load a TSV file of vectors.” and choose “vecs.tsv” file.\n",
    "# Select “Choose file” in “Load a TSV file of metadata.” and choose “meta.tsv” file.\n",
    "# The model has been visualized in 3D/2D.\n",
    "# It is done on choosing variable with higher variance as a dimension from the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis\n",
    "It is performed using Principal Component Analysis(PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "#Computing the correlation matrix\n",
    "M_corr=df.corr()\n",
    "\n",
    "#Computing eigen values and eigen vectors\n",
    "values,vectors=np.linalg.eig(M_corr)\n",
    "\n",
    "\n",
    "# Sorting the eigen vectors coresponding to eigen values in descending order is done like \n",
    "# this to have us choosing best eigenvector for infogain. But in our model its already \n",
    "# sorted the way it should be. Eigenvector with highest eigenvalue in the first column.\n",
    "# args = (-values).argsort()\n",
    "# values = vectors[args]\n",
    "# vectors = vectors[:, args]\n",
    "\n",
    "# our aim is to cover maximum variance possible \n",
    "# no. of components selected = no. of plotting dimensions\n",
    "tot = sum(values) # summation of eigenvalues\n",
    "print(\"\\n\",tot) \n",
    "var_exp = [(i / tot)*100 for i in values[:5]] # first 5 variance in desc order\n",
    "print(\"\\n1. Variance Explained\\n\",var_exp) \n",
    "cum_var_exp = np.cumsum(var_exp) # first 5 cumulative variance\n",
    "print(\"\\n2. Cumulative Variance Explained\\n\",cum_var_exp) \n",
    "\n",
    "#Taking first 2 components which explain maximum variance for projecting\n",
    "new_vectors=vectors[:,:2]\n",
    "\n",
    "new_M=np.dot(M,new_vectors)\n",
    "print(\"\\n\",new_M) # coordinates for 2D plot\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.title('Variances of various components',size=15)\n",
    "plt.xlabel(\"Components\",size=15)\n",
    "plt.ylabel('Variance',size=15)\n",
    "plt.bar(range(1,6),\n",
    "        var_exp,\n",
    "        width=0.8,\n",
    "        color = cm.rainbow(np.linspace(0, 1, len(var_exp))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Plot\n",
    "Using PCA we have done plotting of the vectors in a 2-dimentional space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting\n",
    "plt.figure(figsize=(15,12))\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.scatter(new_M[:,0],new_M[:,1],linewidths=1,color='skyblue',alpha=0.75)\n",
    "plt.xlabel(\"PC1\",size=15)\n",
    "plt.ylabel(\"PC2\",size=15)\n",
    "plt.title(\"Word Embedding Space\",size=20)\n",
    "vocab=list(visualmodel.wv.vocab)\n",
    "for i, word in enumerate(vocab):\n",
    "  if i%37==0:\n",
    "      plt.annotate(word,xy=(new_M[i,0],new_M[i,1])) # selective annotations\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model \n",
    "We have used three types of models namely:\n",
    "Dual layer LSTM, Bi-directional LSTM, Convolutional neural network LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, Conv1D\n",
    "from keras.models import Sequential\n",
    "import keras.backend as K\n",
    "\n",
    "# bidirectional is for evaluating input in both sequential order\n",
    "# conv1D is for adding a Cnn layer instead of a LSTM layer\n",
    "# return sequences return sequences in shape for next Lstm layer\n",
    "# dropout layers are for regularization\n",
    "# dropout is for inputs and recurrent dropout is for recurrent inputs\n",
    "# instead of using sigmoid activation in the output layer we will use Relu since we are not normalising training labels.\n",
    "# x is 0-> Dual layer LSTM, 1-> Bidirectional LSTM, 2-> CNN LSTM \n",
    "\n",
    "def get_model(x):\n",
    "    model = Sequential()\n",
    "    if x == 0:\n",
    "        model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    if x == 1:\n",
    "        model.add(Bidirectional(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True)))\n",
    "    if x == 2:\n",
    "        model.add(Conv1D(64, 3, activation='relu',input_shape=(1,300),padding='same'))\n",
    "        model.add(Dropout(0.4))\n",
    "    model.add(LSTM(128, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu')) \n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "    if x == 1:\n",
    "        model.build((None,1,300))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase\n",
    "Here we will train our data for 5 types of word embeddings and 3 types of LSTM Networks. And store the respective Kappa Scores in the dataframe and them will do comparative analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "    \n",
    "X_train,X_test,y_train,y_test = train_test_split(X, Y, train_size=0.70,test_size=0.30, random_state=0)\n",
    "print('\\nShape of X_train and y_train respectively.\\n')\n",
    "print (X_train.shape, y_train.shape)\n",
    "print (X_test.shape, y_test.shape)\n",
    "    \n",
    "train_essays = X_train['essay']\n",
    "test_essays = X_test['essay']\n",
    "   \n",
    "sentences = []\n",
    "    \n",
    "for essay in train_essays:\n",
    "    # Obtaining all sentences from the training essays.\n",
    "    sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "\n",
    "models = ['Dual-LSTM','Bi-LSTM','Cnn-LSTM']\n",
    "embeddings = ['W2V','W2VP','GloVe','FsTxT','FsTxTP']\n",
    "\n",
    "# dataframe to store our final result\n",
    "final = {\n",
    "    embeddings[0]:{models[0]:0,models[1]:0,models[2]:0},\n",
    "    embeddings[1]:{models[0]:0,models[1]:0,models[2]:0},\n",
    "    embeddings[2]:{models[0]:0,models[1]:0,models[2]:0},\n",
    "    embeddings[3]:{models[0]:0,models[1]:0,models[2]:0},\n",
    "    embeddings[4]:{models[0]:0,models[1]:0,models[2]:0}     \n",
    "}\n",
    "\n",
    "for i in range(5): # for every word embedding\n",
    "    \n",
    "    if(i==0):\n",
    "        model = Word2Vec(\n",
    "            sentences, \n",
    "            workers=num_workers, \n",
    "            size=num_features, \n",
    "            min_count = min_word_count, \n",
    "            window = context, \n",
    "            sample = downsampling\n",
    "        )\n",
    "        model.init_sims(replace=True) # for normalizing\n",
    "    \n",
    "    if(i==1):\n",
    "        model = modelbasic\n",
    "        modelbasic.train(allsentences,total_examples=len(allsentences), epochs=modelbasic.epochs)\n",
    "        modelbasic.init_sims(replace=True) # for normalizing\n",
    "        modelbasic.save(\"./word2vec/word2vecPre.model\") # saving model\n",
    "        model.train(sentences,total_examples=len(sentences), epochs=model.epochs)\n",
    "        model.init_sims(replace=True) # for normalizing\n",
    "    \n",
    "    if(i==2):\n",
    "        model = modelglove\n",
    "        modelglove.train(allsentences,total_examples=len(allsentences), epochs=modelbasic.epochs)\n",
    "        modelglove.init_sims(replace=True) # for normalizing\n",
    "        modelglove.save(\"./glove/gloveW2V.model\") # saving model\n",
    "        model.train(sentences,total_examples=len(sentences), epochs=model.epochs)\n",
    "        model.init_sims(replace=True) # for normalizing\n",
    "    \n",
    "    if(i==3):\n",
    "        model = FastText(\n",
    "            sentences, \n",
    "            workers=num_workers, \n",
    "            size=num_features, \n",
    "            min_count = min_word_count, \n",
    "            window = context, \n",
    "            sample = downsampling\n",
    "        )\n",
    "        model.init_sims(replace=True) # for normalizing\n",
    "    \n",
    "    if(i==4):\n",
    "        model = ftmodelPre\n",
    "        model.train(sentences,total_examples=len(sentences), epochs=model.epochs)\n",
    "        model.init_sims(replace=True) # for normalizing\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    clean_train_essays = []\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "\n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "\n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    \n",
    "    # Reshaping train and test vectors to 3 dimensions for LSTM (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "\n",
    "    for j in range(3): # for every model\n",
    "\n",
    "        print('\\n------------------- {} Model with {} Embeddings -------------------\\n'.format(models[j],embeddings[i]))\n",
    "\n",
    "        lstm_model = get_model(j)\n",
    "        history = lstm_model.fit(\n",
    "            trainDataVecs, \n",
    "            y_train, \n",
    "            validation_data=(testDataVecs,y_test), \n",
    "            batch_size=32, \n",
    "            epochs=100, \n",
    "            shuffle = False\n",
    "        ) \n",
    "        # fitting of the model\n",
    "\n",
    "        # This can be used for prediction\n",
    "        # lstm_model.load_weights('./final_lstm.h5') To load model weight\n",
    "        # Predicting from test data\n",
    "        y_pred = np.around(lstm_model.predict(testDataVecs))\n",
    "        # lstm_model.save('./final_lstm.h5')\n",
    "        \n",
    "        # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "        result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "        final[embeddings[i]][models[j]] = result\n",
    "        \n",
    "        print()\n",
    "        plt.figure(figsize=(10,7))\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('Training vs Validation Loss', size=25)\n",
    "        plt.ylabel('Loss', size=25)\n",
    "        plt.xlabel('No. Of Epochs', size=25)\n",
    "        plt.legend(['Training','Validation'], loc= 'upper right' )\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nKappa Score: {}\".format(result))\n",
    "        print('\\n------------------- {} Model with {} Embeddings -------------------\\n'.format(models[j],embeddings[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Comparison\n",
    "Here we compare our kappa score for different LSTM - Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreDb = pd.DataFrame(final)\n",
    "\n",
    "print('\\nFinal Scores Matrix: \\n')\n",
    "print(scoreDb)\n",
    "print()\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.xlabel(\"Word Embeddings\",size=20)\n",
    "plt.ylabel(\"Kappa Score\",size=20)\n",
    "plt.title(\"Scores For Different LSTM Models\",size=20)\n",
    "color = ['red','blue','green']\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.tick_params(labelsize=15)\n",
    "for i in range(3):\n",
    "    plt.plot(scoreDb.columns.values,scoreDb.iloc[i].values,color=color[i], marker='o', linestyle='--', markersize=10)\n",
    "plt.legend(scoreDb.index.values, fontsize =15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Multinomial Naive Bayes from Scratch\n",
    "Implementing multinomial naive bayes on the basic of word frequency count considering only top 3000 frequent words in the whole corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as ma\n",
    "import itertools # this is to slice the dictionary to get only max frequecvy values\n",
    "from nltk.tokenize import word_tokenize # used to tokenize the sentences\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def probability(dictionary,x,score): # it returns the actual probability of input x over class clas\n",
    "\n",
    "    count = ma.log(dictionary[score][\"count\"]) - ma.log(dictionary[\"total\"]) # it is probability for a certain score\n",
    "    features_number = len(dictionary[score].keys()) - 2 # total number of features\n",
    "    for j in range(features_number): # calculting the probabilty over each feature the later we will take log() sum of all       \n",
    "        \n",
    "        if(x[j]==0): # if input x have zero frequency over the feature so its probability will not counted\n",
    "            continue\n",
    "        \n",
    "        count_xj_in_feature_j = dictionary[score][j] + 1 # it is the total frequency of feature j in a score\n",
    "        # adding one to eliminate any domain specific errors\n",
    "        count_clas_ele_in_feature = dictionary[score][\"Grand_total\"] # it is total number of words in a score\n",
    "        p = ma.log(count_xj_in_feature_j) - ma.log(count_clas_ele_in_feature) \n",
    "        # summing all small probabilities of all features\n",
    "        count = count + p\n",
    "    return count # returning the probabilty\n",
    "\n",
    "def singlecol(dictionary,x): # singlecol gives the prediction(output) of single colum at a time\n",
    "    \n",
    "    best_prob = -1 # giving any value to initialise best_prob\n",
    "    best_cls = -1 # giving any value to initialise best_cls\n",
    "    classes = dictionary.keys() # dictionary .keys have all the classes names\n",
    "    val = True\n",
    "    for clas in classes: # checking probabily on one class at a time \n",
    "        \n",
    "        if (clas==\"total\"): # total is not a class so ignore it\n",
    "            continue\n",
    "        \n",
    "        clas_p = probability(dictionary,x,clas) # clas_p will have probability of input x for class clas\n",
    "        \n",
    "        if(val or clas_p>best_prob):\n",
    "            best_prob = clas_p\n",
    "            best_cls = clas\n",
    "        \n",
    "        val = False\n",
    "    return best_cls\n",
    "\n",
    "X_naive = X['essay'].tolist()\n",
    "Y_naive = X['domain1_score'].tolist()\n",
    "sets = X['essay_set'].tolist()\n",
    "\n",
    "# normalizing score\n",
    "for i in range(len(sets)):\n",
    "    Y_naive[i] = Y_naive[i] - minimum_scores[sets[i]]\n",
    "    Y_naive[i] = int(np.around((Y_naive[i] * 5) / (maximum_scores[sets[i]] - minimum_scores[sets[i]])))\n",
    "\n",
    "# doing hold out spliting for train and test data\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(X_naive,Y_naive,test_size=0.3,random_state=0)\n",
    "\n",
    "len_data = len(xtrain)\n",
    "\n",
    "# in this dictionary we will store frequency of each word from entire dataset by removing stop_words\n",
    "dictionary = dict()\n",
    "for j in range(len_data):\n",
    "    data = xtrain[j]\n",
    "    word_tokens = word_tokenize(data)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    \n",
    "    for word in filtered_sentence:\n",
    "        if word in dictionary:\n",
    "            dictionary[word]+=1\n",
    "        else:\n",
    "            dictionary[word]=1\n",
    "\n",
    "new_dict={} # this is the reverse sorted form of dictionary used above\n",
    "for key,value in sorted(dictionary.items(),key=lambda item: item[1],reverse=True):\n",
    "    new_dict[key]=value\n",
    "\n",
    "# slicing over bigger ditionary to get max  frequency 3000 data only\n",
    "a=dict(itertools.islice(new_dict.items(),3000))\n",
    "     \n",
    "features=[] # features is the list of keys of dictionary (a) \n",
    "for i in a.keys():\n",
    "    features.append(i)\n",
    "\n",
    "# modifing x_train and x_test to 2d Lists having frequency of each word of features \n",
    "xx_train=np.zeros((len(xtrain),len(features)))\n",
    "for i in range(len(xtrain)):\n",
    "    data=xtrain[i]\n",
    "    \n",
    "    word_tokens = word_tokenize(data)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    for j in filtered_sentence:\n",
    "        if j in features:\n",
    "            xx_train[i][features.index(j)]+=1\n",
    "            \n",
    "xx_test=np.zeros((len(xtest),len(features)))\n",
    "for i in range(len(xtest)):\n",
    "    data2 = xtest[i]\n",
    "    word_tok = word_tokenize(data2)\n",
    "    \n",
    "    fil_sentence = [w for w in word_tok if not w in stop_words]\n",
    "    \n",
    "    for j in fil_sentence:\n",
    "        if j in features:\n",
    "            xx_test[i][features.index(j)]+=1\n",
    "\n",
    "# this is to train algorithm over training data\n",
    "result = {} # we will use dictionary and create nested dictionary where needed\n",
    "classes = set(ytrain)\n",
    "\n",
    "for current_class in classes: # accessing all score classes one by one \n",
    "    x_train_current = []   \n",
    "    y_train_current = []\n",
    "    result[current_class] = {}\n",
    "    result[\"total\"] = len(xx_train) # it will hold length of entire xx_train set\n",
    "    for i in range(len(ytrain)):\n",
    "\n",
    "        if (Y_naive[i]==current_class):\n",
    "            x_train_current.append(xx_train[i])\n",
    "            y_train_current = Y_naive[i] # spliting y_train for only current_class\n",
    "    \n",
    "    result[current_class][\"count\"] = len(x_train_current) # it will hold count of current_class\n",
    "    features_total = xx_train.shape[1] # feature size is nothin but the columns of xx_train\n",
    "    a = 0\n",
    "    for j in range(len(features)):\n",
    "        result[current_class][j]=0\n",
    "        for k in range(len(x_train_current)):\n",
    "            result[current_class][j] += x_train_current[k][j] # it will hold frequency of feature j\n",
    "        a += result[current_class][j]\n",
    "    result[current_class][\"Grand_total\"] = a  # it will hold count of entire words in current_class\n",
    "    \n",
    "ypred = []\n",
    "for x in xx_test: # going through test_data row wise\n",
    "    pred=singlecol(result,x) # as we get a answer by one column we are appending it to list\n",
    "    ypred.append(pred)    \n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "#importing these to check correctness of y_pred(output)\n",
    "print('\\nThis classification is due to our implementation\\n')\n",
    "print(classification_report(ytest,ypred))\n",
    "print('\\n---------------------COMPARISION---------------------\\n')\n",
    "print('\\nThis classification is due to sklearn library\\n')\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB # now doing the same fit and predict by MultinomialNB library function\n",
    "arg1=MultinomialNB()\n",
    "arg1.fit(xx_train,ytrain)\n",
    "ypred2=arg1.predict(xx_test)\n",
    "\n",
    "print(classification_report(ytest,ypred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Naive Bayes \n",
    "The brighter colors and higher value number around the diagonal shows the accuracy of our model and dispersion shows the low precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns # for seaborn \n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "array = confusion_matrix(ytest,ypred2)\n",
    "df_cm = pd.DataFrame(array, range(6), range(6))\n",
    "fig = sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 16},fmt='d') # font size\n",
    "sns.set(font_scale=1.4) # for label size\n",
    "plt.xlabel(\"Predicted Score\", size=25)\n",
    "plt.ylabel(\"Actual Score\", size=25)\n",
    "plt.tick_params(labelsize=10)\n",
    "plt.title(\"Confusion matrix for Multinomial Naive Bayes\")\n",
    "plt.show(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
