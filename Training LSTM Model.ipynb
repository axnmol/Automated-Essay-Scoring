{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR = './data/' # Datasets to be places here\n",
    "SAVE_DIR = './' # Main Dir\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Removing empty columns and finding minimum and maximum scores given to each of the 8 sets\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])\n",
    "Z = pd.read_excel(r'./data/essay_set_descriptions.xlsx')\n",
    "minimum_scores = Z['min_domain1_score'].to_list()\n",
    "minimum_scores.insert(0,-1)\n",
    "maximum_scores = Z['max_domain1_score'].to_list()\n",
    "maximum_scores.insert(0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   essay_id  essay_set                                              essay  \\\n0         1          1  Dear local newspaper, I think effects computer...   \n1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n4         5          1  Dear @LOCATION1, I know having computers has a...   \n\n   domain1_score  \n0              8  \n1              9  \n2              7  \n3             10  \n4              8  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>essay_set</th>\n      <th>essay</th>\n      <th>domain1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Dear local newspaper, I think effects computer...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>Dear @LOCATION1, I know having computers has a...</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-1, 2, 1, 0, 0, 0, 0, 0, 0]\n[-1, 12, 6, 3, 3, 4, 4, 30, 60]\n"
    }
   ],
   "source": [
    "print(minimum_scores)\n",
    "print(maximum_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of the Data\n",
    "\n",
    "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
    "\n",
    "These are all helper functions used to clean the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bce7dc1b3f3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;31m# will be returning list of words without stopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmakeFeatureVec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# !pip install gensim\n",
    "# !pip install nltk\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('popular')\n",
    "import re # for regular expressions operations \n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v) # removing anything that is not alphabetic\n",
    "    words = essay_v.lower().split() # turn sentence into lowercase and split it into words \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\")) # english stopwords library \n",
    "        words = [w for w in words if not w in stops] # words present in the sentence and not present in stopwords\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') # nltk library\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip()) # call tokenizer on essay striped of spaces \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "            # remove_stopwords carried forward to essay_to_wordlist it is a bool variable\n",
    "    return sentences # will be returning list of words without stopwords\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n",
    "\n",
    "Note that instead of using sigmoid activation in the output layer we will use\n",
    "Relu since we are not normalising training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tep - loss: 10.1608 - mae: 1.7738\nEpoch 26/50\n163/163 [==============================] - 19s 119ms/step - loss: 9.9542 - mae: 1.7754\nEpoch 27/50\n163/163 [==============================] - 7s 40ms/step - loss: 10.5233 - mae: 1.7871\nEpoch 28/50\n163/163 [==============================] - 6s 39ms/step - loss: 10.1837 - mae: 1.7630\nEpoch 29/50\n163/163 [==============================] - 6s 39ms/step - loss: 9.2500 - mae: 1.7365\nEpoch 30/50\n163/163 [==============================] - 6s 36ms/step - loss: 9.5618 - mae: 1.7460\nEpoch 31/50\n163/163 [==============================] - 6s 38ms/step - loss: 9.4187 - mae: 1.7308\nEpoch 32/50\n163/163 [==============================] - 7s 40ms/step - loss: 9.4168 - mae: 1.7261\nEpoch 33/50\n163/163 [==============================] - 6s 37ms/step - loss: 9.4949 - mae: 1.7148\nEpoch 34/50\n163/163 [==============================] - 6s 39ms/step - loss: 9.2357 - mae: 1.7051\nEpoch 35/50\n163/163 [==============================] - 17s 107ms/step - loss: 8.9750 - mae: 1.6997\nEpoch 36/50\n163/163 [==============================] - 6s 38ms/step - loss: 9.1303 - mae: 1.6954\nEpoch 37/50\n163/163 [==============================] - 6s 36ms/step - loss: 9.1210 - mae: 1.6847\nEpoch 38/50\n163/163 [==============================] - 6s 39ms/step - loss: 8.6700 - mae: 1.6704\nEpoch 39/50\n163/163 [==============================] - 6s 39ms/step - loss: 8.9334 - mae: 1.6707\nEpoch 40/50\n163/163 [==============================] - 6s 35ms/step - loss: 9.0240 - mae: 1.6858\nEpoch 41/50\n163/163 [==============================] - 6s 35ms/step - loss: 8.7984 - mae: 1.6651\nEpoch 42/50\n163/163 [==============================] - 6s 36ms/step - loss: 8.7711 - mae: 1.6668\nEpoch 43/50\n163/163 [==============================] - 6s 39ms/step - loss: 8.5844 - mae: 1.6484\nEpoch 44/50\n163/163 [==============================] - 6s 37ms/step - loss: 8.4120 - mae: 1.6271\nEpoch 45/50\n163/163 [==============================] - 6s 37ms/step - loss: 8.5697 - mae: 1.6571\nEpoch 46/50\n163/163 [==============================] - 7s 44ms/step - loss: 8.6425 - mae: 1.6547\nEpoch 47/50\n163/163 [==============================] - 9s 58ms/step - loss: 8.1733 - mae: 1.6233\nEpoch 48/50\n163/163 [==============================] - 13s 81ms/step - loss: 8.4489 - mae: 1.6479\nEpoch 49/50\n163/163 [==============================] - 7s 41ms/step - loss: 8.1331 - mae: 1.6190\nEpoch 50/50\n163/163 [==============================] - 7s 41ms/step - loss: 8.0171 - mae: 1.6212\nKappa Score: 0.9547837498886934\n\n--------Fold 3--------\n\nTraining Word2Vec Model...\nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_4 (LSTM)                (None, 1, 300)            721200    \n_________________________________________________________________\nlstm_5 (LSTM)                (None, 64)                93440     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 814,705\nTrainable params: 814,705\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/50\n163/163 [==============================] - 7s 42ms/step - loss: 63.3074 - mae: 4.3520\nEpoch 2/50\n163/163 [==============================] - 5s 30ms/step - loss: 40.2631 - mae: 3.6297\nEpoch 3/50\n163/163 [==============================] - 6s 35ms/step - loss: 33.8528 - mae: 3.4901\nEpoch 4/50\n163/163 [==============================] - 12s 74ms/step - loss: 30.5165 - mae: 3.3988\nEpoch 5/50\n163/163 [==============================] - 6s 39ms/step - loss: 28.5337 - mae: 3.2475\nEpoch 6/50\n163/163 [==============================] - 7s 43ms/step - loss: 27.0543 - mae: 3.0687\nEpoch 7/50\n163/163 [==============================] - 17s 102ms/step - loss: 24.6798 - mae: 2.8689\nEpoch 8/50\n163/163 [==============================] - 7s 42ms/step - loss: 21.6546 - mae: 2.6727\nEpoch 9/50\n163/163 [==============================] - 7s 44ms/step - loss: 18.6979 - mae: 2.4805\nEpoch 10/50\n163/163 [==============================] - 6s 37ms/step - loss: 16.7353 - mae: 2.3541\nEpoch 11/50\n163/163 [==============================] - 6s 40ms/step - loss: 16.2186 - mae: 2.2984\nEpoch 12/50\n163/163 [==============================] - 7s 44ms/step - loss: 15.0528 - mae: 2.1861\nEpoch 13/50\n163/163 [==============================] - 7s 42ms/step - loss: 13.8895 - mae: 2.1285\nEpoch 14/50\n163/163 [==============================] - 6s 38ms/step - loss: 14.1930 - mae: 2.1172\nEpoch 15/50\n163/163 [==============================] - 6s 36ms/step - loss: 13.4029 - mae: 2.0694\nEpoch 16/50\n163/163 [==============================] - 6s 36ms/step - loss: 13.2469 - mae: 2.0417\nEpoch 17/50\n163/163 [==============================] - 8s 52ms/step - loss: 12.6771 - mae: 1.9967\nEpoch 18/50\n163/163 [==============================] - 11s 66ms/step - loss: 12.2132 - mae: 1.9515\nEpoch 19/50\n163/163 [==============================] - 7s 45ms/step - loss: 12.5110 - mae: 1.9627\nEpoch 20/50\n163/163 [==============================] - 13s 81ms/step - loss: 11.7471 - mae: 1.9161\nEpoch 21/50\n163/163 [==============================] - 6s 34ms/step - loss: 10.8526 - mae: 1.8674\nEpoch 22/50\n163/163 [==============================] - 5s 34ms/step - loss: 11.1306 - mae: 1.8381\nEpoch 23/50\n163/163 [==============================] - 6s 34ms/step - loss: 10.5730 - mae: 1.8174\nEpoch 24/50\n163/163 [==============================] - 6s 34ms/step - loss: 10.6508 - mae: 1.8216\nEpoch 25/50\n163/163 [==============================] - 6s 34ms/step - loss: 10.4690 - mae: 1.8038\nEpoch 26/50\n163/163 [==============================] - 12s 73ms/step - loss: 10.7404 - mae: 1.7978\nEpoch 27/50\n163/163 [==============================] - 6s 39ms/step - loss: 10.5667 - mae: 1.7937\nEpoch 28/50\n163/163 [==============================] - 44s 269ms/step - loss: 10.1920 - mae: 1.7707\nEpoch 29/50\n163/163 [==============================] - 31s 193ms/step - loss: 10.1069 - mae: 1.7584\nEpoch 30/50\n163/163 [==============================] - 14s 85ms/step - loss: 9.9811 - mae: 1.7262\nEpoch 31/50\n163/163 [==============================] - 6s 37ms/step - loss: 9.9434 - mae: 1.7585\nEpoch 32/50\n163/163 [==============================] - 6s 38ms/step - loss: 9.6554 - mae: 1.7302\nEpoch 33/50\n163/163 [==============================] - 11s 66ms/step - loss: 9.7988 - mae: 1.7374\nEpoch 34/50\n163/163 [==============================] - 6s 39ms/step - loss: 9.5611 - mae: 1.7224\nEpoch 35/50\n163/163 [==============================] - 6s 39ms/step - loss: 9.5373 - mae: 1.7157\nEpoch 36/50\n163/163 [==============================] - 7s 40ms/step - loss: 8.9414 - mae: 1.6858\nEpoch 37/50\n163/163 [==============================] - 6s 39ms/step - loss: 9.1850 - mae: 1.6962\nEpoch 38/50\n163/163 [==============================] - 6s 40ms/step - loss: 9.2964 - mae: 1.7114\nEpoch 39/50\n163/163 [==============================] - 7s 42ms/step - loss: 8.5158 - mae: 1.6613\nEpoch 40/50\n163/163 [==============================] - 7s 42ms/step - loss: 8.8652 - mae: 1.6755\nEpoch 41/50\n163/163 [==============================] - 8s 47ms/step - loss: 8.6546 - mae: 1.6631\nEpoch 42/50\n163/163 [==============================] - 13s 80ms/step - loss: 8.7219 - mae: 1.6723\nEpoch 43/50\n163/163 [==============================] - 7s 41ms/step - loss: 8.9842 - mae: 1.6697\nEpoch 44/50\n163/163 [==============================] - 6s 39ms/step - loss: 8.6506 - mae: 1.6567\nEpoch 45/50\n163/163 [==============================] - 17s 104ms/step - loss: 8.7381 - mae: 1.6555\nEpoch 46/50\n163/163 [==============================] - 11s 65ms/step - loss: 8.5668 - mae: 1.6532\nEpoch 47/50\n163/163 [==============================] - 9s 58ms/step - loss: 8.5126 - mae: 1.6603\nEpoch 48/50\n163/163 [==============================] - 4s 27ms/step - loss: 8.3224 - mae: 1.6388\nEpoch 49/50\n163/163 [==============================] - 4s 25ms/step - loss: 8.4501 - mae: 1.6351\nEpoch 50/50\n163/163 [==============================] - 4s 25ms/step - loss: 8.4274 - mae: 1.6363\nKappa Score: 0.9613108175093871\n\n--------Fold 4--------\n\nTraining Word2Vec Model...\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_6 (LSTM)                (None, 1, 300)            721200    \n_________________________________________________________________\nlstm_7 (LSTM)                (None, 64)                93440     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 814,705\nTrainable params: 814,705\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/50\n163/163 [==============================] - 3s 21ms/step - loss: 61.6048 - mae: 4.2917\nEpoch 2/50\n163/163 [==============================] - 4s 23ms/step - loss: 39.9078 - mae: 3.6090\nEpoch 3/50\n163/163 [==============================] - 4s 25ms/step - loss: 33.7634 - mae: 3.5037\nEpoch 4/50\n163/163 [==============================] - 5s 31ms/step - loss: 30.7884 - mae: 3.4179\nEpoch 5/50\n163/163 [==============================] - 6s 36ms/step - loss: 29.1884 - mae: 3.2957\nEpoch 6/50\n163/163 [==============================] - 6s 34ms/step - loss: 27.8663 - mae: 3.1115\nEpoch 7/50\n163/163 [==============================] - 3s 20ms/step - loss: 25.6171 - mae: 2.9321\nEpoch 8/50\n163/163 [==============================] - 3s 19ms/step - loss: 22.3677 - mae: 2.7370\nEpoch 9/50\n163/163 [==============================] - 3s 19ms/step - loss: 20.3886 - mae: 2.5645\nEpoch 10/50\n163/163 [==============================] - 3s 20ms/step - loss: 17.8758 - mae: 2.4067\nEpoch 11/50\n163/163 [==============================] - 3s 19ms/step - loss: 16.7457 - mae: 2.3308\nEpoch 12/50\n163/163 [==============================] - 3s 19ms/step - loss: 15.9561 - mae: 2.2669\nEpoch 13/50\n163/163 [==============================] - 4s 22ms/step - loss: 14.9764 - mae: 2.1742\nEpoch 14/50\n163/163 [==============================] - 3s 21ms/step - loss: 14.3201 - mae: 2.1316\nEpoch 15/50\n163/163 [==============================] - 3s 19ms/step - loss: 13.7837 - mae: 2.0984\nEpoch 16/50\n163/163 [==============================] - 3s 19ms/step - loss: 13.2117 - mae: 2.0421\nEpoch 17/50\n163/163 [==============================] - 3s 21ms/step - loss: 12.6213 - mae: 2.0046\nEpoch 18/50\n163/163 [==============================] - 3s 19ms/step - loss: 12.7292 - mae: 1.9949\nEpoch 19/50\n163/163 [==============================] - 3s 19ms/step - loss: 11.8733 - mae: 1.9633\nEpoch 20/50\n163/163 [==============================] - 3s 18ms/step - loss: 12.4523 - mae: 1.9482\nEpoch 21/50\n163/163 [==============================] - 3s 18ms/step - loss: 11.7304 - mae: 1.8939\nEpoch 22/50\n163/163 [==============================] - 3s 18ms/step - loss: 11.5215 - mae: 1.8793\nEpoch 23/50\n163/163 [==============================] - 3s 18ms/step - loss: 11.0188 - mae: 1.8538\nEpoch 24/50\n163/163 [==============================] - 3s 19ms/step - loss: 11.1585 - mae: 1.8426\nEpoch 25/50\n163/163 [==============================] - 3s 19ms/step - loss: 10.9534 - mae: 1.8273\nEpoch 26/50\n163/163 [==============================] - 3s 20ms/step - loss: 10.3904 - mae: 1.8022\nEpoch 27/50\n163/163 [==============================] - 3s 21ms/step - loss: 10.7535 - mae: 1.8096\nEpoch 28/50\n163/163 [==============================] - 3s 20ms/step - loss: 10.2026 - mae: 1.7994\nEpoch 29/50\n163/163 [==============================] - 3s 20ms/step - loss: 10.2285 - mae: 1.7806\nEpoch 30/50\n163/163 [==============================] - 4s 23ms/step - loss: 9.5712 - mae: 1.7465\nEpoch 31/50\n163/163 [==============================] - 4s 22ms/step - loss: 9.8439 - mae: 1.7635\nEpoch 32/50\n163/163 [==============================] - 3s 20ms/step - loss: 9.6094 - mae: 1.7418\nEpoch 33/50\n163/163 [==============================] - 3s 19ms/step - loss: 9.8164 - mae: 1.7501\nEpoch 34/50\n163/163 [==============================] - 3s 19ms/step - loss: 9.8296 - mae: 1.7479\nEpoch 35/50\n163/163 [==============================] - 4s 22ms/step - loss: 9.4331 - mae: 1.7150\nEpoch 36/50\n163/163 [==============================] - 3s 21ms/step - loss: 9.5012 - mae: 1.7252\nEpoch 37/50\n163/163 [==============================] - 3s 21ms/step - loss: 9.7207 - mae: 1.7346\nEpoch 38/50\n163/163 [==============================] - 3s 20ms/step - loss: 9.4337 - mae: 1.7048\nEpoch 39/50\n163/163 [==============================] - 3s 20ms/step - loss: 9.2116 - mae: 1.6868\nEpoch 40/50\n163/163 [==============================] - 3s 19ms/step - loss: 9.3645 - mae: 1.7058\nEpoch 41/50\n163/163 [==============================] - 3s 20ms/step - loss: 9.1092 - mae: 1.6986\nEpoch 42/50\n163/163 [==============================] - 3s 19ms/step - loss: 8.8710 - mae: 1.6786\nEpoch 43/50\n163/163 [==============================] - 3s 19ms/step - loss: 9.0030 - mae: 1.6812\nEpoch 44/50\n163/163 [==============================] - 3s 20ms/step - loss: 8.9302 - mae: 1.6773\nEpoch 45/50\n163/163 [==============================] - 4s 22ms/step - loss: 9.0839 - mae: 1.6757\nEpoch 46/50\n163/163 [==============================] - 5s 29ms/step - loss: 8.8805 - mae: 1.6636\nEpoch 47/50\n163/163 [==============================] - 5s 31ms/step - loss: 8.6216 - mae: 1.6492\nEpoch 48/50\n163/163 [==============================] - 5s 29ms/step - loss: 8.4757 - mae: 1.6413\nEpoch 49/50\n163/163 [==============================] - 8s 47ms/step - loss: 8.7987 - mae: 1.6551\nEpoch 50/50\n163/163 [==============================] - 5s 32ms/step - loss: 8.4847 - mae: 1.6385\nKappa Score: 0.9628906298812432\n\n--------Fold 5--------\n\nTraining Word2Vec Model...\nModel: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_8 (LSTM)                (None, 1, 300)            721200    \n_________________________________________________________________\nlstm_9 (LSTM)                (None, 64)                93440     \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 814,705\nTrainable params: 814,705\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/50\n163/163 [==============================] - 3s 18ms/step - loss: 68.6981 - mae: 4.4481\nEpoch 2/50\n163/163 [==============================] - 4s 22ms/step - loss: 42.3453 - mae: 3.6439\nEpoch 3/50\n163/163 [==============================] - 4s 22ms/step - loss: 34.3481 - mae: 3.5032\nEpoch 4/50\n163/163 [==============================] - 3s 19ms/step - loss: 30.9693 - mae: 3.3788\nEpoch 5/50\n163/163 [==============================] - 3s 20ms/step - loss: 28.6738 - mae: 3.2020\nEpoch 6/50\n163/163 [==============================] - 4s 23ms/step - loss: 26.6599 - mae: 3.0075\nEpoch 7/50\n163/163 [==============================] - 3s 18ms/step - loss: 23.3137 - mae: 2.7850\nEpoch 8/50\n163/163 [==============================] - 5s 28ms/step - loss: 21.8527 - mae: 2.6692\nEpoch 9/50\n163/163 [==============================] - 4s 25ms/step - loss: 18.7602 - mae: 2.4757\nEpoch 10/50\n163/163 [==============================] - 4s 26ms/step - loss: 17.8751 - mae: 2.3843\nEpoch 11/50\n163/163 [==============================] - 3s 19ms/step - loss: 15.6582 - mae: 2.2482\nEpoch 12/50\n163/163 [==============================] - 4s 27ms/step - loss: 15.0408 - mae: 2.2046\nEpoch 13/50\n163/163 [==============================] - 5s 32ms/step - loss: 14.0390 - mae: 2.1093\nEpoch 14/50\n163/163 [==============================] - 5s 30ms/step - loss: 14.1513 - mae: 2.1191\nEpoch 15/50\n163/163 [==============================] - 3s 21ms/step - loss: 12.9942 - mae: 2.0284\nEpoch 16/50\n163/163 [==============================] - 3s 18ms/step - loss: 12.4522 - mae: 1.9959\nEpoch 17/50\n163/163 [==============================] - 3s 18ms/step - loss: 12.7610 - mae: 1.9785\nEpoch 18/50\n163/163 [==============================] - 5s 31ms/step - loss: 11.6189 - mae: 1.9144\nEpoch 19/50\n163/163 [==============================] - 3s 21ms/step - loss: 11.2200 - mae: 1.8931\nEpoch 20/50\n163/163 [==============================] - 4s 22ms/step - loss: 11.3153 - mae: 1.8703\nEpoch 21/50\n163/163 [==============================] - 4s 22ms/step - loss: 11.3435 - mae: 1.8675\nEpoch 22/50\n163/163 [==============================] - 4s 25ms/step - loss: 10.4786 - mae: 1.8051\nEpoch 23/50\n163/163 [==============================] - 4s 24ms/step - loss: 10.5190 - mae: 1.7896\nEpoch 24/50\n163/163 [==============================] - 3s 18ms/step - loss: 10.0536 - mae: 1.7558\nEpoch 25/50\n163/163 [==============================] - 3s 18ms/step - loss: 10.2688 - mae: 1.7801\nEpoch 26/50\n163/163 [==============================] - 3s 21ms/step - loss: 10.5561 - mae: 1.7799\nEpoch 27/50\n163/163 [==============================] - 3s 18ms/step - loss: 10.1077 - mae: 1.7611\nEpoch 28/50\n163/163 [==============================] - 3s 18ms/step - loss: 9.9905 - mae: 1.7471\nEpoch 29/50\n163/163 [==============================] - 3s 19ms/step - loss: 9.6384 - mae: 1.7386\nEpoch 30/50\n163/163 [==============================] - 3s 18ms/step - loss: 9.6288 - mae: 1.7330\nEpoch 31/50\n163/163 [==============================] - 3s 18ms/step - loss: 9.3627 - mae: 1.7206\nEpoch 32/50\n163/163 [==============================] - 3s 18ms/step - loss: 9.5387 - mae: 1.7309\nEpoch 33/50\n163/163 [==============================] - 3s 20ms/step - loss: 9.2340 - mae: 1.7037\nEpoch 34/50\n163/163 [==============================] - 6s 36ms/step - loss: 9.1682 - mae: 1.7049\nEpoch 35/50\n163/163 [==============================] - 6s 37ms/step - loss: 9.2094 - mae: 1.7007\nEpoch 36/50\n163/163 [==============================] - 5s 29ms/step - loss: 9.4385 - mae: 1.7005\nEpoch 37/50\n163/163 [==============================] - 5s 32ms/step - loss: 9.1687 - mae: 1.6808\nEpoch 38/50\n163/163 [==============================] - 4s 22ms/step - loss: 8.6043 - mae: 1.6519\nEpoch 39/50\n163/163 [==============================] - 4s 22ms/step - loss: 8.4725 - mae: 1.6564\nEpoch 40/50\n163/163 [==============================] - 4s 22ms/step - loss: 8.8003 - mae: 1.6675\nEpoch 41/50\n163/163 [==============================] - 4s 22ms/step - loss: 8.8777 - mae: 1.6710\nEpoch 42/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.8193 - mae: 1.6430\nEpoch 43/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.6790 - mae: 1.6612\nEpoch 44/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.7419 - mae: 1.6712\nEpoch 45/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.5462 - mae: 1.6365\nEpoch 46/50\n163/163 [==============================] - 4s 22ms/step - loss: 8.5601 - mae: 1.6373\nEpoch 47/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.2071 - mae: 1.6182\nEpoch 48/50\n163/163 [==============================] - 4s 22ms/step - loss: 8.4015 - mae: 1.6402\nEpoch 49/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.8582 - mae: 1.6455\nEpoch 50/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.5654 - mae: 1.6456\n"
    },
    {
     "output_type": "error",
     "ename": "OSError",
     "evalue": "Unable to create file (unable to open file: name = './model_weights/final_lstm.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-fdfb9bae8527>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m# Save any one of the 8 models.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m          \u001b[0mlstm_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./model_weights/final_lstm.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;31m# Round y_pred to the nearest integer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m   1976\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1977\u001b[0m     \"\"\"\n\u001b[1;32m-> 1978\u001b[1;33m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0m\u001b[0;32m   1979\u001b[0m                     signatures, options)\n\u001b[0;32m   1980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\save.py\u001b[0m in \u001b[0;36msave_model\u001b[1;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options)\u001b[0m\n\u001b[0;32m    128\u001b[0m           \u001b[1;34m'to the Tensorflow SavedModel format (by setting save_format=\"tf\") '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m           'or using `save_weights`.')\n\u001b[1;32m--> 130\u001b[1;33m     hdf5_format.save_model_to_hdf5(\n\u001b[0m\u001b[0;32m    131\u001b[0m         model, filepath, overwrite, include_optimizer)\n\u001b[0;32m    132\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\saving\\hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[1;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    404\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrdcc_nslots\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrdcc_nbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrdcc_w0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0m\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                                swmr=swmr)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Open in append mode (read/write).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to create file (unable to open file: name = './model_weights/final_lstm.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 302)"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "cv.get_n_splits(X)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "            \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    clean_train_essays = []\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./model_weights/final_lstm.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Avg. Kappa Score is 0.961 which is the highest we have ever seen on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average Kappa score after a 5-fold cross validation:  0.9591\n"
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}