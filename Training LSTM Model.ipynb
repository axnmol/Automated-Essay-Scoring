{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR = './data/' # Datasets to be places here\n",
    "SAVE_DIR = './' # Main Dir\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Removing empty columns and finding minimum and maximum scores given to each of the 8 sets\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "# Reading from tsv file (tab - separated) with Latin alphabet encoding including special symbols\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])\n",
    "Z = pd.read_excel(r'./data/essay_set_descriptions.xlsx')\n",
    "minimum_scores = Z['min_domain1_score'].to_list()\n",
    "minimum_scores.insert(0,-1)\n",
    "maximum_scores = Z['max_domain1_score'].to_list()\n",
    "maximum_scores.insert(0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   essay_id  essay_set                                              essay  \\\n0         1          1  Dear local newspaper, I think effects computer...   \n1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n4         5          1  Dear @LOCATION1, I know having computers has a...   \n\n   domain1_score  \n0              8  \n1              9  \n2              7  \n3             10  \n4              8  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>essay_id</th>\n      <th>essay_set</th>\n      <th>essay</th>\n      <th>domain1_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>Dear local newspaper, I think effects computer...</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n      <td>10</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1</td>\n      <td>Dear @LOCATION1, I know having computers has a...</td>\n      <td>8</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-1, 2, 1, 0, 0, 0, 0, 0, 0]\n[-1, 12, 6, 3, 3, 4, 4, 30, 60]\n"
    }
   ],
   "source": [
    "print(minimum_scores)\n",
    "print(maximum_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing of the Data\n",
    "\n",
    "These are all helper functions used to clean and tokenize the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install gensim\n",
    "# !pip install nltk\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('popular')\n",
    "import re # for regular expressions operations \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    # remove the tagged labels and word tokenize the sentence\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v) # removing anything that is not alphabetic\n",
    "    words = essay_v.lower().split() # turn sentence into lowercase and split it into words \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\")) # english stopwords library \n",
    "        words = [w for w in words if not w in stops] # words present in the sentence and not present in stopwords\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    # sentence tokenize the essay and call essay_to_wordlist() for word tokenization\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') # nltk library\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip()) # call tokenizer on essay striped of spaces \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "            # remove_stopwords carried forward to essay_to_wordlist it is a bool variable\n",
    "    return sentences # will be returning list of tokenized sentences without stopwords"
   ]
  },
  {
   "source": [
    "## Word2Vec Model\n",
    "\n",
    "We will preprocess all essays and convert them to feature vectors and will visualize them using tensorflow"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-09-25 02:32:26,625 - dictionary - INFO - adding document #0 to Dictionary(0 unique tokens: [])\n2020-09-25 02:32:26,630 - dictionary - INFO - built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n2020-09-25 02:32:26,636 - word2vec - INFO - collecting all words and their counts\n2020-09-25 02:32:26,644 - word2vec - INFO - PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n2020-09-25 02:32:26,734 - word2vec - INFO - PROGRESS: at sentence #10000, processed 81383 words, keeping 7084 word types\n2020-09-25 02:32:26,802 - word2vec - INFO - PROGRESS: at sentence #20000, processed 162931 words, keeping 10507 word types\n2020-09-25 02:32:26,841 - word2vec - INFO - PROGRESS: at sentence #30000, processed 243216 words, keeping 13069 word types\n2020-09-25 02:32:26,879 - word2vec - INFO - PROGRESS: at sentence #40000, processed 327952 words, keeping 15868 word types\n2020-09-25 02:32:26,956 - word2vec - INFO - PROGRESS: at sentence #50000, processed 414213 words, keeping 18172 word types\n2020-09-25 02:32:27,063 - word2vec - INFO - PROGRESS: at sentence #60000, processed 499427 words, keeping 20089 word types\n2020-09-25 02:32:27,131 - word2vec - INFO - PROGRESS: at sentence #70000, processed 587249 words, keeping 22586 word types\n2020-09-25 02:32:27,216 - word2vec - INFO - PROGRESS: at sentence #80000, processed 680425 words, keeping 24581 word types\n2020-09-25 02:32:27,268 - word2vec - INFO - PROGRESS: at sentence #90000, processed 774481 words, keeping 26002 word types\n2020-09-25 02:32:27,329 - word2vec - INFO - PROGRESS: at sentence #100000, processed 872053 words, keeping 27538 word types\n2020-09-25 02:32:27,368 - word2vec - INFO - PROGRESS: at sentence #110000, processed 936551 words, keeping 29962 word types\n2020-09-25 02:32:27,431 - word2vec - INFO - PROGRESS: at sentence #120000, processed 1014437 words, keeping 31798 word types\n2020-09-25 02:32:27,514 - word2vec - INFO - PROGRESS: at sentence #130000, processed 1095909 words, keeping 33188 word types\n2020-09-25 02:32:27,533 - word2vec - INFO - collected 33443 word types from a corpus of 1111441 raw words and 131775 sentences\n2020-09-25 02:32:27,536 - word2vec - INFO - Loading a fresh vocabulary\n2020-09-25 02:32:27,594 - word2vec - INFO - effective_min_count=40 retains 2577 unique words (7% of original 33443, drops 30866)\n2020-09-25 02:32:27,596 - word2vec - INFO - effective_min_count=40 leaves 998680 word corpus (89% of original 1111441, drops 112761)\n2020-09-25 02:32:27,631 - word2vec - INFO - deleting the raw counts dictionary of 33443 items\n2020-09-25 02:32:27,634 - word2vec - INFO - sample=0.001 downsamples 66 most-common words\n2020-09-25 02:32:27,640 - word2vec - INFO - downsampling leaves estimated 877096 word corpus (87.8% of prior 998680)\n2020-09-25 02:32:27,660 - base_any2vec - INFO - estimated required memory for 2577 words and 300 dimensions: 7473300 bytes\n2020-09-25 02:32:27,661 - word2vec - INFO - resetting layer weights\n2020-09-25 02:32:29,281 - base_any2vec - INFO - training model with 4 workers on 2577 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n2020-09-25 02:32:30,310 - base_any2vec - INFO - EPOCH 1 - PROGRESS: at 52.46% examples, 448304 words/s, in_qsize 7, out_qsize 0\n2020-09-25 02:32:31,274 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n2020-09-25 02:32:31,276 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n2020-09-25 02:32:31,290 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n2020-09-25 02:32:31,297 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n2020-09-25 02:32:31,298 - base_any2vec - INFO - EPOCH - 1 : training on 1111441 raw words (877167 effective words) took 2.0s, 439948 effective words/s\n2020-09-25 02:32:32,421 - base_any2vec - INFO - EPOCH 2 - PROGRESS: at 37.56% examples, 283657 words/s, in_qsize 3, out_qsize 10\n2020-09-25 02:32:33,454 - base_any2vec - INFO - EPOCH 2 - PROGRESS: at 95.31% examples, 391642 words/s, in_qsize 4, out_qsize 2\n2020-09-25 02:32:33,458 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n2020-09-25 02:32:33,464 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n2020-09-25 02:32:33,479 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n2020-09-25 02:32:33,491 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n2020-09-25 02:32:33,492 - base_any2vec - INFO - EPOCH - 2 : training on 1111441 raw words (877040 effective words) took 2.2s, 402994 effective words/s\n2020-09-25 02:32:34,544 - base_any2vec - INFO - EPOCH 3 - PROGRESS: at 45.56% examples, 372861 words/s, in_qsize 7, out_qsize 0\n2020-09-25 02:32:35,625 - base_any2vec - INFO - EPOCH 3 - PROGRESS: at 85.94% examples, 360391 words/s, in_qsize 7, out_qsize 7\n2020-09-25 02:32:35,741 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n2020-09-25 02:32:35,759 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n2020-09-25 02:32:35,769 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n2020-09-25 02:32:35,785 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n2020-09-25 02:32:35,787 - base_any2vec - INFO - EPOCH - 3 : training on 1111441 raw words (877372 effective words) took 2.3s, 385402 effective words/s\n2020-09-25 02:32:36,833 - base_any2vec - INFO - EPOCH 4 - PROGRESS: at 41.14% examples, 341718 words/s, in_qsize 6, out_qsize 1\n2020-09-25 02:32:37,869 - base_any2vec - INFO - EPOCH 4 - PROGRESS: at 83.82% examples, 364916 words/s, in_qsize 7, out_qsize 4\n2020-09-25 02:32:38,089 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n2020-09-25 02:32:38,090 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n2020-09-25 02:32:38,104 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n2020-09-25 02:32:38,107 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n2020-09-25 02:32:38,109 - base_any2vec - INFO - EPOCH - 4 : training on 1111441 raw words (877075 effective words) took 2.3s, 383601 effective words/s\n2020-09-25 02:32:39,235 - base_any2vec - INFO - EPOCH 5 - PROGRESS: at 44.72% examples, 344182 words/s, in_qsize 7, out_qsize 5\n2020-09-25 02:32:40,351 - base_any2vec - INFO - EPOCH 5 - PROGRESS: at 82.61% examples, 334347 words/s, in_qsize 8, out_qsize 5\n2020-09-25 02:32:40,487 - base_any2vec - INFO - worker thread finished; awaiting finish of 3 more threads\n2020-09-25 02:32:40,498 - base_any2vec - INFO - worker thread finished; awaiting finish of 2 more threads\n2020-09-25 02:32:40,508 - base_any2vec - INFO - worker thread finished; awaiting finish of 1 more threads\n2020-09-25 02:32:40,508 - base_any2vec - INFO - worker thread finished; awaiting finish of 0 more threads\n2020-09-25 02:32:40,510 - base_any2vec - INFO - EPOCH - 5 : training on 1111441 raw words (876897 effective words) took 2.4s, 369947 effective words/s\n2020-09-25 02:32:40,512 - base_any2vec - INFO - training on a 5557205 raw words (4385551 effective words) took 11.2s, 390574 effective words/s\n2020-09-25 02:32:40,517 - keyedvectors - INFO - precomputing L2-norms of word weight vectors\n2020-09-25 02:32:40,523 - utils_any2vec - INFO - storing 2577x300 projection weights into word2vecvisual.txt\n2020-09-25 02:32:42,026 - utils_any2vec - INFO - storing 2577x300 projection weights into word2vecvisual.bin\n2020-09-25 02:32:42,077 - utils - INFO - saving Word2Vec object under word2vecvisual,model, separately None\n2020-09-25 02:32:42,079 - utils - INFO - not storing attribute vectors_norm\n2020-09-25 02:32:42,082 - utils - INFO - not storing attribute cum_table\n2020-09-25 02:32:42,262 - utils - INFO - saved word2vecvisual,model\n"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec # for word2vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #  make null vectors of length = num_features for every word index\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word) # vocabulary set of model\n",
    "    for word in words:\n",
    "        if word in index2word_set: # for word in a sentence being in index2word_set\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    if num_words:\n",
    "        featureVec = np.divide(featureVec,num_words) \n",
    "        # divides every element of featureVec by num_words (to compute average) \n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    # it then calls makeFeatureVec and for every essay sentance\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    # make null vectors of length = length of essay having null vector of num_features at every index\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs\n",
    "    # then returns the trained model feature vectors\n",
    "    \n",
    "allsentences = [] # list of all sentences\n",
    "    \n",
    "for essay in X['essay']:\n",
    "# obtaining all sentences from the essays.\n",
    "    allsentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "            \n",
    "# initializing variables for word2vec model.\n",
    "num_features = 300 # vector length\n",
    "min_word_count = 40 # to be considered for vectorisation\n",
    "num_workers = 8 # working cores\n",
    "context = 10\n",
    "downsampling = 1e-3 # compressing\n",
    "\n",
    "# for visualization of vectors\n",
    "visualmodel = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "path = get_tmpfile(\"word2vecvisual.model\") # path for the model \n",
    "\n",
    "visualmodel.init_sims(replace=True) # normalized model\n",
    "# different ways to save model\n",
    "visualmodel.wv.save_word2vec_format('word2vecvisual.txt', binary=False)\n",
    "visualmodel.wv.save_word2vec_format('word2vecvisual.bin', binary=True)\n",
    "visualmodel.save(\"word2vecvisual,model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "2020-09-25 02:22:31,709 - utils_any2vec - INFO - loading projection weights from .//word2vecmodel.bin\n2020-09-25 02:22:31,788 - utils_any2vec - INFO - loaded (2591, 300) matrix from .//word2vecmodel.bin\n"
    }
   ],
   "source": [
    "import io # for input output\n",
    "\n",
    "# for visualization of a stored model\n",
    "'''from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format(SAVE_DIR+'/word2vecmodel.bin', binary=True)'''\n",
    "\n",
    "# files needed for tensorboard\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "# write meta file and vector file\n",
    "for index in range(len(model.index2word)): # for every word in vocab\n",
    "    word = model.index2word[index]\n",
    "    vec = model.vectors[index]\n",
    "    out_m.write(word + \"\\n\")\n",
    "    out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()\n",
    "\n",
    "'''Open http://projector.tensorflow.org/\n",
    "   Click “Load Data” button from the left menu.\n",
    "   Select “Choose file” in “Load a TSV file of vectors.” and choose “vecs.tsv” file.\n",
    "   Select “Choose file” in “Load a TSV file of metadata.” and choose “meta.tsv” file.\n",
    "   The model has been visualized in 3D/2D.\n",
    "   It is done by choosing variable with higher variance as a dimension from the vector'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n",
    "\n",
    "Note that instead of using sigmoid activation in the output layer we will use\n",
    "Relu since we are not normalising training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4108 - mae: 1.7962\nEpoch 26/50\n163/163 [==============================] - 4s 23ms/step - loss: 11.1056 - mae: 1.8006\nEpoch 27/50\n163/163 [==============================] - 3s 18ms/step - loss: 10.4238 - mae: 1.7695\nEpoch 28/50\n163/163 [==============================] - 4s 26ms/step - loss: 10.0927 - mae: 1.7582\nEpoch 29/50\n163/163 [==============================] - 3s 20ms/step - loss: 10.7142 - mae: 1.7908\nEpoch 30/50\n163/163 [==============================] - 3s 19ms/step - loss: 10.1332 - mae: 1.7479\nEpoch 31/50\n163/163 [==============================] - 3s 20ms/step - loss: 10.1616 - mae: 1.7618\nEpoch 32/50\n163/163 [==============================] - 3s 21ms/step - loss: 9.7996 - mae: 1.7333\nEpoch 33/50\n163/163 [==============================] - 3s 21ms/step - loss: 10.0107 - mae: 1.7396\nEpoch 34/50\n163/163 [==============================] - 4s 23ms/step - loss: 9.4298 - mae: 1.7278\nEpoch 35/50\n163/163 [==============================] - 3s 21ms/step - loss: 9.5925 - mae: 1.7309\nEpoch 36/50\n163/163 [==============================] - 4s 22ms/step - loss: 9.4080 - mae: 1.7085\nEpoch 37/50\n163/163 [==============================] - 5s 33ms/step - loss: 9.3840 - mae: 1.7035\nEpoch 38/50\n163/163 [==============================] - 4s 24ms/step - loss: 9.2310 - mae: 1.6969\nEpoch 39/50\n163/163 [==============================] - 4s 23ms/step - loss: 8.8964 - mae: 1.6818\nEpoch 40/50\n163/163 [==============================] - 4s 22ms/step - loss: 9.4322 - mae: 1.6896\nEpoch 41/50\n163/163 [==============================] - 4s 24ms/step - loss: 9.0651 - mae: 1.6734\nEpoch 42/50\n163/163 [==============================] - 3s 21ms/step - loss: 9.1785 - mae: 1.6765\nEpoch 43/50\n163/163 [==============================] - 4s 23ms/step - loss: 8.8901 - mae: 1.6592\nEpoch 44/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.7575 - mae: 1.6480\nEpoch 45/50\n163/163 [==============================] - 3s 17ms/step - loss: 9.1676 - mae: 1.6844\nEpoch 46/50\n163/163 [==============================] - 3s 19ms/step - loss: 8.5930 - mae: 1.6396\nEpoch 47/50\n163/163 [==============================] - 3s 17ms/step - loss: 8.8713 - mae: 1.6629\nEpoch 48/50\n163/163 [==============================] - 4s 23ms/step - loss: 8.6200 - mae: 1.6340\nEpoch 49/50\n163/163 [==============================] - 4s 23ms/step - loss: 8.4138 - mae: 1.6216\nEpoch 50/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.5086 - mae: 1.6328\nKappa Score: 0.9587212384584866\n\n--------Fold 3--------\n\nTraining Word2Vec Model...\nModel: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_4 (LSTM)                (None, 1, 300)            721200    \n_________________________________________________________________\nlstm_5 (LSTM)                (None, 64)                93440     \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 814,705\nTrainable params: 814,705\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/50\n163/163 [==============================] - 4s 22ms/step - loss: 62.1735 - mae: 4.3114\nEpoch 2/50\n163/163 [==============================] - 5s 33ms/step - loss: 39.9196 - mae: 3.5929\nEpoch 3/50\n163/163 [==============================] - 5s 28ms/step - loss: 34.4193 - mae: 3.4590\nEpoch 4/50\n163/163 [==============================] - 4s 22ms/step - loss: 31.0474 - mae: 3.3643\nEpoch 5/50\n163/163 [==============================] - 4s 22ms/step - loss: 29.5562 - mae: 3.2459\nEpoch 6/50\n163/163 [==============================] - 4s 23ms/step - loss: 27.2180 - mae: 3.0728\nEpoch 7/50\n163/163 [==============================] - 4s 24ms/step - loss: 25.9118 - mae: 2.9306\nEpoch 8/50\n163/163 [==============================] - 3s 20ms/step - loss: 21.9054 - mae: 2.7031\nEpoch 9/50\n163/163 [==============================] - 3s 21ms/step - loss: 19.2069 - mae: 2.5218\nEpoch 10/50\n163/163 [==============================] - 3s 21ms/step - loss: 17.3489 - mae: 2.4157\nEpoch 11/50\n163/163 [==============================] - 3s 19ms/step - loss: 15.4630 - mae: 2.2737\nEpoch 12/50\n163/163 [==============================] - 3s 20ms/step - loss: 14.8975 - mae: 2.2050\nEpoch 13/50\n163/163 [==============================] - 3s 21ms/step - loss: 14.5650 - mae: 2.1413\nEpoch 14/50\n163/163 [==============================] - 3s 17ms/step - loss: 13.7298 - mae: 2.0992\nEpoch 15/50\n163/163 [==============================] - 3s 19ms/step - loss: 13.6200 - mae: 2.0825\nEpoch 16/50\n163/163 [==============================] - 3s 17ms/step - loss: 12.4735 - mae: 1.9846\nEpoch 17/50\n163/163 [==============================] - 3s 20ms/step - loss: 12.1389 - mae: 1.9625\nEpoch 18/50\n163/163 [==============================] - 3s 20ms/step - loss: 12.3522 - mae: 1.9614\nEpoch 19/50\n163/163 [==============================] - 3s 20ms/step - loss: 11.5740 - mae: 1.8927\nEpoch 20/50\n163/163 [==============================] - 3s 21ms/step - loss: 11.9420 - mae: 1.9182\nEpoch 21/50\n163/163 [==============================] - 3s 20ms/step - loss: 11.0580 - mae: 1.8602\nEpoch 22/50\n163/163 [==============================] - 3s 20ms/step - loss: 10.8373 - mae: 1.8271\nEpoch 23/50\n163/163 [==============================] - 4s 22ms/step - loss: 10.6400 - mae: 1.8073\nEpoch 24/50\n163/163 [==============================] - 3s 20ms/step - loss: 10.5913 - mae: 1.8049\nEpoch 25/50\n163/163 [==============================] - 3s 20ms/step - loss: 10.2765 - mae: 1.7790\nEpoch 26/50\n163/163 [==============================] - 4s 24ms/step - loss: 10.5609 - mae: 1.7879\nEpoch 27/50\n163/163 [==============================] - 4s 23ms/step - loss: 10.1766 - mae: 1.7778\nEpoch 28/50\n163/163 [==============================] - 4s 22ms/step - loss: 10.5447 - mae: 1.7680\nEpoch 29/50\n163/163 [==============================] - 3s 20ms/step - loss: 9.7992 - mae: 1.7411\nEpoch 30/50\n163/163 [==============================] - 5s 30ms/step - loss: 9.8351 - mae: 1.7412\nEpoch 31/50\n163/163 [==============================] - 4s 22ms/step - loss: 9.4256 - mae: 1.7220\nEpoch 32/50\n163/163 [==============================] - 3s 17ms/step - loss: 9.8731 - mae: 1.7391\nEpoch 33/50\n163/163 [==============================] - 3s 21ms/step - loss: 9.3817 - mae: 1.7181\nEpoch 34/50\n163/163 [==============================] - 3s 18ms/step - loss: 9.5567 - mae: 1.7299\nEpoch 35/50\n163/163 [==============================] - 3s 19ms/step - loss: 9.8376 - mae: 1.7334\nEpoch 36/50\n163/163 [==============================] - 3s 20ms/step - loss: 8.9320 - mae: 1.6909\nEpoch 37/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.9113 - mae: 1.6728\nEpoch 38/50\n163/163 [==============================] - 3s 20ms/step - loss: 9.6020 - mae: 1.7232\nEpoch 39/50\n163/163 [==============================] - 4s 22ms/step - loss: 8.8762 - mae: 1.6633\nEpoch 40/50\n163/163 [==============================] - 3s 18ms/step - loss: 8.8718 - mae: 1.6718\nEpoch 41/50\n163/163 [==============================] - 3s 18ms/step - loss: 8.5016 - mae: 1.6411\nEpoch 42/50\n163/163 [==============================] - 3s 18ms/step - loss: 8.5012 - mae: 1.6500\nEpoch 43/50\n163/163 [==============================] - 4s 24ms/step - loss: 8.7356 - mae: 1.6787\nEpoch 44/50\n163/163 [==============================] - 3s 19ms/step - loss: 8.6777 - mae: 1.6589\nEpoch 45/50\n163/163 [==============================] - 4s 25ms/step - loss: 8.8004 - mae: 1.6584\nEpoch 46/50\n163/163 [==============================] - 4s 22ms/step - loss: 8.3496 - mae: 1.6272\nEpoch 47/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.2925 - mae: 1.6405\nEpoch 48/50\n163/163 [==============================] - 4s 23ms/step - loss: 8.3762 - mae: 1.6371\nEpoch 49/50\n163/163 [==============================] - 4s 21ms/step - loss: 8.4547 - mae: 1.6246\nEpoch 50/50\n163/163 [==============================] - 4s 26ms/step - loss: 8.4912 - mae: 1.6406\nKappa Score: 0.9552350009153312\n\n--------Fold 4--------\n\nTraining Word2Vec Model...\nModel: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_6 (LSTM)                (None, 1, 300)            721200    \n_________________________________________________________________\nlstm_7 (LSTM)                (None, 64)                93440     \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 814,705\nTrainable params: 814,705\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/50\n163/163 [==============================] - 4s 24ms/step - loss: 63.5257 - mae: 4.3663\nEpoch 2/50\n163/163 [==============================] - 5s 32ms/step - loss: 39.9025 - mae: 3.6151\nEpoch 3/50\n163/163 [==============================] - 5s 30ms/step - loss: 33.7560 - mae: 3.4897\nEpoch 4/50\n163/163 [==============================] - 3s 20ms/step - loss: 30.7551 - mae: 3.3830\nEpoch 5/50\n163/163 [==============================] - 3s 18ms/step - loss: 28.6437 - mae: 3.2216\nEpoch 6/50\n163/163 [==============================] - 3s 18ms/step - loss: 26.6827 - mae: 3.0416\nEpoch 7/50\n163/163 [==============================] - 3s 18ms/step - loss: 25.4449 - mae: 2.8956\nEpoch 8/50\n163/163 [==============================] - 3s 18ms/step - loss: 21.4532 - mae: 2.6509\nEpoch 9/50\n163/163 [==============================] - 3s 17ms/step - loss: 19.4501 - mae: 2.4940\nEpoch 10/50\n163/163 [==============================] - 4s 22ms/step - loss: 17.4994 - mae: 2.3813\nEpoch 11/50\n163/163 [==============================] - 3s 20ms/step - loss: 15.9335 - mae: 2.2906\nEpoch 12/50\n163/163 [==============================] - 4s 25ms/step - loss: 15.7383 - mae: 2.2426\nEpoch 13/50\n163/163 [==============================] - 3s 19ms/step - loss: 15.3967 - mae: 2.1976\nEpoch 14/50\n163/163 [==============================] - 3s 18ms/step - loss: 14.5427 - mae: 2.1444\nEpoch 15/50\n163/163 [==============================] - 3s 18ms/step - loss: 14.0324 - mae: 2.0970\nEpoch 16/50\n163/163 [==============================] - 4s 22ms/step - loss: 13.3692 - mae: 2.0411\nEpoch 17/50\n163/163 [==============================] - 3s 21ms/step - loss: 12.6252 - mae: 2.0162\nEpoch 18/50\n163/163 [==============================] - 3s 21ms/step - loss: 13.4801 - mae: 2.0159\nEpoch 19/50\n163/163 [==============================] - 3s 18ms/step - loss: 12.0083 - mae: 1.9387\nEpoch 20/50\n163/163 [==============================] - 3s 18ms/step - loss: 12.3356 - mae: 1.9481\nEpoch 21/50\n163/163 [==============================] - 3s 18ms/step - loss: 11.8673 - mae: 1.9182\nEpoch 22/50\n163/163 [==============================] - 3s 20ms/step - loss: 11.7919 - mae: 1.8853\nEpoch 23/50\n163/163 [==============================] - 3s 20ms/step - loss: 11.0810 - mae: 1.8576\nEpoch 24/50\n163/163 [==============================] - 4s 22ms/step - loss: 11.4243 - mae: 1.8690\nEpoch 25/50\n163/163 [==============================] - 4s 24ms/step - loss: 10.9604 - mae: 1.8286\nEpoch 26/50\n163/163 [==============================] - 5s 28ms/step - loss: 10.8786 - mae: 1.8150\nEpoch 27/50\n163/163 [==============================] - 4s 26ms/step - loss: 11.0446 - mae: 1.8366\nEpoch 28/50\n163/163 [==============================] - 3s 19ms/step - loss: 10.2768 - mae: 1.8003\nEpoch 29/50\n163/163 [==============================] - 3s 21ms/step - loss: 10.7001 - mae: 1.8132\nEpoch 30/50\n163/163 [==============================] - 3s 20ms/step - loss: 10.5125 - mae: 1.7977\nEpoch 31/50\n163/163 [==============================] - 3s 19ms/step - loss: 9.4079 - mae: 1.7496\nEpoch 32/50\n163/163 [==============================] - 3s 20ms/step - loss: 10.6302 - mae: 1.8043\nEpoch 33/50\n163/163 [==============================] - 3s 17ms/step - loss: 9.5661 - mae: 1.7370\nEpoch 34/50\n163/163 [==============================] - 4s 22ms/step - loss: 9.6170 - mae: 1.7185\nEpoch 35/50\n163/163 [==============================] - 3s 19ms/step - loss: 9.4662 - mae: 1.7182\nEpoch 36/50\n163/163 [==============================] - 3s 21ms/step - loss: 9.4566 - mae: 1.7343\nEpoch 37/50\n163/163 [==============================] - 4s 25ms/step - loss: 9.3341 - mae: 1.7204\nEpoch 38/50\n163/163 [==============================] - 3s 20ms/step - loss: 9.6863 - mae: 1.7214\nEpoch 39/50\n163/163 [==============================] - 4s 22ms/step - loss: 9.8425 - mae: 1.7359\nEpoch 40/50\n163/163 [==============================] - 6s 34ms/step - loss: 9.3997 - mae: 1.7165\nEpoch 41/50\n163/163 [==============================] - 4s 23ms/step - loss: 9.1140 - mae: 1.6945\nEpoch 42/50\n163/163 [==============================] - 4s 23ms/step - loss: 9.3173 - mae: 1.7008\nEpoch 43/50\n163/163 [==============================] - 4s 24ms/step - loss: 8.9254 - mae: 1.6896\nEpoch 44/50\n163/163 [==============================] - 4s 23ms/step - loss: 9.4198 - mae: 1.7071\nEpoch 45/50\n163/163 [==============================] - 4s 24ms/step - loss: 8.9085 - mae: 1.6804\nEpoch 46/50\n163/163 [==============================] - 4s 27ms/step - loss: 8.8054 - mae: 1.6786\nEpoch 47/50\n163/163 [==============================] - 4s 22ms/step - loss: 9.2489 - mae: 1.7024\nEpoch 48/50\n163/163 [==============================] - 4s 23ms/step - loss: 9.1104 - mae: 1.6744\nEpoch 49/50\n163/163 [==============================] - 5s 28ms/step - loss: 8.8669 - mae: 1.6811\nEpoch 50/50\n163/163 [==============================] - 4s 27ms/step - loss: 8.9047 - mae: 1.6690\nKappa Score: 0.9589989335171586\n\n--------Fold 5--------\n\nTraining Word2Vec Model...\nModel: \"sequential_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm_8 (LSTM)                (None, 1, 300)            721200    \n_________________________________________________________________\nlstm_9 (LSTM)                (None, 64)                93440     \n_________________________________________________________________\ndropout_4 (Dropout)          (None, 64)                0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 1)                 65        \n=================================================================\nTotal params: 814,705\nTrainable params: 814,705\nNon-trainable params: 0\n_________________________________________________________________\nEpoch 1/50\n163/163 [==============================] - 4s 23ms/step - loss: 63.2088 - mae: 4.3276\nEpoch 2/50\n163/163 [==============================] - 4s 22ms/step - loss: 40.4033 - mae: 3.5847\nEpoch 3/50\n163/163 [==============================] - 3s 20ms/step - loss: 33.9428 - mae: 3.4523\nEpoch 4/50\n163/163 [==============================] - 3s 21ms/step - loss: 30.5943 - mae: 3.3888\nEpoch 5/50\n163/163 [==============================] - 4s 24ms/step - loss: 28.2516 - mae: 3.2174\nEpoch 6/50\n163/163 [==============================] - 3s 20ms/step - loss: 26.3573 - mae: 3.0221\nEpoch 7/50\n163/163 [==============================] - 4s 22ms/step - loss: 24.7845 - mae: 2.8798\nEpoch 8/50\n163/163 [==============================] - 4s 22ms/step - loss: 21.6720 - mae: 2.6862\nEpoch 9/50\n163/163 [==============================] - 3s 20ms/step - loss: 18.7816 - mae: 2.5055\nEpoch 10/50\n163/163 [==============================] - 3s 20ms/step - loss: 16.6650 - mae: 2.3652\nEpoch 11/50\n163/163 [==============================] - 4s 24ms/step - loss: 15.4168 - mae: 2.2637\nEpoch 12/50\n163/163 [==============================] - 4s 24ms/step - loss: 14.4156 - mae: 2.1929\nEpoch 13/50\n163/163 [==============================] - 4s 22ms/step - loss: 13.9094 - mae: 2.1270\nEpoch 14/50\n163/163 [==============================] - 4s 22ms/step - loss: 13.0545 - mae: 2.0719\nEpoch 15/50\n163/163 [==============================] - 3s 20ms/step - loss: 12.7450 - mae: 2.0095\nEpoch 16/50\n163/163 [==============================] - 3s 19ms/step - loss: 12.1877 - mae: 1.9908\nEpoch 17/50\n163/163 [==============================] - 3s 19ms/step - loss: 12.0159 - mae: 1.9571\nEpoch 18/50\n163/163 [==============================] - 3s 18ms/step - loss: 11.6327 - mae: 1.9222\nEpoch 19/50\n163/163 [==============================] - 3s 19ms/step - loss: 11.6597 - mae: 1.9053\nEpoch 20/50\n163/163 [==============================] - 5s 33ms/step - loss: 11.3515 - mae: 1.8923\nEpoch 21/50\n163/163 [==============================] - 6s 35ms/step - loss: 11.0586 - mae: 1.8529\nEpoch 22/50\n163/163 [==============================] - 4s 28ms/step - loss: 10.9151 - mae: 1.8469\nEpoch 23/50\n163/163 [==============================] - 5s 28ms/step - loss: 10.6695 - mae: 1.8186\nEpoch 24/50\n163/163 [==============================] - 4s 26ms/step - loss: 10.3988 - mae: 1.7956\nEpoch 25/50\n163/163 [==============================] - 4s 25ms/step - loss: 10.6528 - mae: 1.8080\nEpoch 26/50\n163/163 [==============================] - 4s 26ms/step - loss: 9.5140 - mae: 1.7497\nEpoch 27/50\n163/163 [==============================] - 4s 26ms/step - loss: 9.5816 - mae: 1.7374\nEpoch 28/50\n163/163 [==============================] - 4s 27ms/step - loss: 9.6138 - mae: 1.7413\nEpoch 29/50\n163/163 [==============================] - 5s 30ms/step - loss: 9.8180 - mae: 1.7398\nEpoch 30/50\n163/163 [==============================] - 4s 26ms/step - loss: 9.3717 - mae: 1.7117\nEpoch 31/50\n163/163 [==============================] - 4s 27ms/step - loss: 9.1908 - mae: 1.7007\nEpoch 32/50\n163/163 [==============================] - 4s 27ms/step - loss: 9.1808 - mae: 1.7140\nEpoch 33/50\n163/163 [==============================] - 4s 24ms/step - loss: 8.7815 - mae: 1.6706\nEpoch 34/50\n163/163 [==============================] - 5s 28ms/step - loss: 9.3171 - mae: 1.7147\nEpoch 35/50\n163/163 [==============================] - 4s 26ms/step - loss: 9.1251 - mae: 1.7098\nEpoch 36/50\n163/163 [==============================] - 4s 24ms/step - loss: 9.3815 - mae: 1.7011\nEpoch 37/50\n163/163 [==============================] - 5s 29ms/step - loss: 9.1208 - mae: 1.7013\nEpoch 38/50\n163/163 [==============================] - 4s 24ms/step - loss: 8.9815 - mae: 1.6766\nEpoch 39/50\n163/163 [==============================] - 4s 24ms/step - loss: 8.9025 - mae: 1.6583\nEpoch 40/50\n163/163 [==============================] - 6s 39ms/step - loss: 9.1128 - mae: 1.6860\nEpoch 41/50\n163/163 [==============================] - 5s 32ms/step - loss: 8.5900 - mae: 1.6446\nEpoch 42/50\n163/163 [==============================] - 4s 27ms/step - loss: 8.6524 - mae: 1.6580\nEpoch 43/50\n163/163 [==============================] - 4s 24ms/step - loss: 8.5907 - mae: 1.6479\nEpoch 44/50\n163/163 [==============================] - 4s 24ms/step - loss: 8.5683 - mae: 1.6402\nEpoch 45/50\n163/163 [==============================] - 4s 24ms/step - loss: 8.7444 - mae: 1.6424\nEpoch 46/50\n163/163 [==============================] - 4s 24ms/step - loss: 8.2061 - mae: 1.6184\nEpoch 47/50\n163/163 [==============================] - 4s 24ms/step - loss: 8.2816 - mae: 1.6182\nEpoch 48/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.3854 - mae: 1.6162\nEpoch 49/50\n163/163 [==============================] - 3s 21ms/step - loss: 8.5037 - mae: 1.5985\nEpoch 50/50\n163/163 [==============================] - 4s 22ms/step - loss: 8.0838 - mae: 1.5898\nKappa Score: 0.9643004059260943\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "cv.get_n_splits(X)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "            \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.txt', binary=False)\n",
    "\n",
    "    clean_train_essays = []\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./model_weights/final_lstm.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Avg. Kappa Score is 0.961 which is the highest we have ever seen on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average Kappa score after a 5-fold cross validation:  0.9588\n"
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}